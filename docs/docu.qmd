---
title: Documentation
---

# 1. Introduction

Ce projet, intitulé **CycleVision3**, vise à analyser le trafic cycliste à Montpellier, en se concentrant particulièrement sur l'étude des trajets effectués via le système de vélos en libre-service, ainsi que sur les données de comptage des cyclistes et des piétons. À travers l'exploitation de plusieurs jeux de données, l'objectif est de créer des visualisations interactives permettant de prédire les tendances du trafic cycliste dans la ville, tout en offrant un accès simplifié à ces analyses via un site web interactif.

L'analyse s'appuie sur trois principales sources de données :

- **VéloMagg** : Un jeu de données contenant des informations sur les trajets effectués avec les vélos en libre-service, fournies par Montpellier Méditerranée Métropole.
- **Comptage des cyclistes et piétons** : Des données issues des compteurs installés pour surveiller le flux de cyclistes et de piétons dans la ville.
- **Données Open Street Map** : Des données géographiques ouvertes, utilisées pour la visualisation cartographique et pour contextualiser les trajets de manière géospatiale.

L'objectif principal de ce projet est de regrouper toutes ces informations et de les rendre accessibles sous forme de visualisations interactives, offrant ainsi un outil puissant pour l'analyse du trafic cycliste et la planification urbaine. L'une des fonctionnalités clés sera la création d'une carte prédictive du trafic cycliste, permettant de prévoir les flux de vélos pour la journée suivante, sur la base des données historiques et des tendances observées.

Ce projet a pour ambition de contribuer à une meilleure compréhension du trafic cycliste dans une grande agglomération et pourrait potentiellement être adapté à d'autres villes, offrant ainsi une solution générique pour l'analyse du trafic cycliste urbain.

Les résultats de ce projet devraient permettre non seulement d'améliorer la gestion du trafic cycliste mais aussi d'offrir aux autorités locales des outils de planification plus précis pour favoriser l'utilisation du vélo comme mode de transport durable.

#### Contexte et objectifs

Le développement de ce projet s'inscrit dans un contexte où les enjeux de mobilité durable et de gestion intelligente des infrastructures urbaines sont de plus en plus cruciaux. L'augmentation de la fréquentation cycliste dans les grandes villes, associée à une volonté d'encourager les comportements écoresponsables, nécessite des outils adaptés pour analyser et prédire les flux de circulation. Ce projet, à travers son site web interactif, cherche à répondre à ces besoins en fournissant des visualisations précises et en temps réel du trafic cycliste.

# 2. Project Setup

- **Traffic Analysis**: Analyze bike traffic data to identify trends, peak times, and popular routes.
- **Interactive Visualization**: Create dynamic maps that visualize traffic flows across Montpellier.
- **Web Integration**: Develop a user-friendly website to showcase data insights and traffic maps.

## 2.1 Structure du projet

La structure du projet est organisée de manière à assurer une séparation claire des différentes composantes nécessaires au bon déroulement de l'analyse et à la création des visualisations. Voici un aperçu de l'organisation des fichiers et répertoires :

<details> <summary>Voir le structure</summary>

```
main/                                           # Répertoire principal du projet
├── .github/workflows/                          # 
├── analyse_donnee/                             # 
├── cache/                                      # Fichiers temporaires
├── data/                                       # Stockage des données des 3 jeux de données
│   ├── extracted/                              # 
│   ├── CoursesVelomagg.csv                     # Données sur les trajets de vélos en libre-service
│   ├── video.csv                               # 
│   └── video_avec_coord.csv                    # 
├── docs/                                       # Répertoire des fichiers du site web
│   ├── .gitignore                              # Liste des fichiers à ignorer par Git
│   ├──                                         # 
│   ├──                                         # 
│   ├──                                         # 
│   └── styles.css                              # Fichier CSS pour la mise en forme du site
├── images/                                     # Images utilisées dans le projet
├── map/                                        # 
├── roadmap/                                    # Fichier README.md avec la description .......
├── slide/                                      # 
├── src/                                        # Répertoire contenant le code source
│   ├── __init__.py                             # Initialisation du package
│   ├── donnée.py                               # Contient les fonctions liées aux données
│   └── fonctions_basedonnees.py                # Fonctions pour les opérations sur la base de données
├── vidéo/                                      # 
├── visualisation/                              # Résultats des visualisations
├──.DS_Store                                    # 
├──
├──
├── .Rhistory                                   # Fichier d'historique R
├── .gitignore                                  # Fichier Git ignore
├── README.md                                   # Documentation principale du projet
└── requirements.txt                            # Dépendances du projet
```
</details>

## 2.2 Installation et dépendances

Avant de commencer le développement et l'analyse des données, il est nécessaire de configurer l'environnement de travail en installant toutes les dépendances nécessaires. Voici les étapes à suivre pour préparer le projet :

1. **Cloner le dépôt Git** : 
   La première étape consiste à cloner le dépôt du projet depuis GitHub à l'aide de la commande suivante :
   ```bash
   git clone https://github.com/mscaia/PROJ_HAX712X.git
   ```
   
2. **Installer les dépendances** :
   Le projet repose sur plusieurs bibliothèques Python pour l'analyse des données, la création de visualisations interactives et la génération du site web. Pour installer ces bibliothèques, utilisez la commande suivante :
   ```bash
   pip install -r requirements.txt
   ```

3. **Vérification de l'environnement** :
   Après l'installation des dépendances, vous pouvez vérifier que tout fonctionne correctement en exécutant un script de test ou en accédant au site web via `quarto` :
   ```bash
   quarto preview
   ```
   Cela lancera un serveur local où vous pourrez visualiser le site et tester les fonctionnalités interactives.

## 2.3 Configuration des fichiers

Le projet est configuré de manière à utiliser **Quarto** pour la création du site web interactif et **Sphinx** pour la documentation générée automatiquement. Le fichier `_quarto.yml` contient les paramètres de configuration du projet Quarto, tandis que le fichier `index.qmd` contient le contenu principal de la page d'accueil du site.

## 2.4 Base de données et traitement des données

Le répertoire `data/` contient les trois principaux jeux de données utilisés dans ce projet :

- **CoursesVelomagg.csv** : Contient les informations sur les trajets effectués avec les vélos en libre-service.
- **EcoCompt1.json** : Données de comptage des cyclistes et piétons provenant des capteurs installés dans la ville.
- **GeolocCompteurs.csv** : Données géospatiales pour localiser les capteurs de comptage des vélos et piétons.

Le traitement de ces données est effectué dans le répertoire `src/`, où les scripts comme `donnée.py` et `fonctions_basedonnees.py` contiennent les fonctions nécessaires pour charger, nettoyer et manipuler les données.

## 2.5 Exécution du projet

Une fois l'environnement configuré et les données traitées, il suffit de lancer le script principal pour générer les visualisations interactives et les cartes prédictives du trafic cycliste. Le projet utilise **Folium** pour la visualisation sur carte et **Matplotlib** pour les graphiques statistiques.

Le répertoire `visualisation/` contient les scripts et sorties graphiques générées au fur et à mesure du traitement des données, notamment les cartes des trajets cyclistes et les prévisions de trafic.

## 2.6 Documentation et tests

Pour garantir la qualité du code et la conformité aux exigences du projet, des tests unitaires sont fournis pour vérifier le bon fonctionnement des différentes fonctionnalités. La documentation technique est générée via **Sphinx** et **Quarto**, offrant ainsi une vue d'ensemble sur la structure du code, les API disponibles et la méthodologie utilisée.


# 3. Data Description

Our analysis relies on three primary datasets:

- **VéloMagg Bike-Sharing Data**: Information on bike-sharing usage within the Montpellier Méditerranée Métropole area.
- **Traffic Counts Data**: Sensor data recording bike and pedestrian counts at various locations throughout the city.
- **OpenStreetMap (OSM) Data**: Geographic data used to accurately map streets, bike lanes, and important locations.

- [Trajets à vélos en libre-service](https://data.montpellier3m.fr/dataset/courses-des-velos-velomagg-de-montpellier-mediterranee-metropole)
- [Comptages vélo et piéton issus des compteurs de vélo](https://data.montpellier3m.fr/dataset/comptages-velo-et-pieton-issus-des-compteurs-de-velo)
- [OpenStreetMap](https://www.openstreetmap.org/#map=6/46.45/2.21)

# 4. Project Workflow

## 4.1 Data Cleaning and Preprocessing

### Tasks:
- **Data Integrity**: Clean the data by removing anomalies, handling missing values, and ensuring consistent formatting.
- **Date and Address Formatting**: Convert date fields for temporal analysis and standardize address formatting to enable efficient location-based processing.

## 4.2 Traffic Visualization and Mapping

### Tasks:
- **Map Generation**: Use the Folium library to generate an interactive map of Montpellier, displaying bike stations, routes, and traffic density.
- **Route Calculation**: Utilize OSMnx to calculate the routes between bike stations, highlighting the most popular paths and enabling traffic analysis over time.

## 4.3 Website Development

The final deliverable will be an interactive website, presenting all the data visualizations and analytical insights. The website will be developed using Quarto for easy integration of analysis and visualizations into an accessible, interactive interface.

# 5. Data Processing Pipeline

# 6. Technical Documentation

## 6.1 Bibliothèques utilisées

Dans le cadre de ce projet, plusieurs bibliothèques ont été utilisées pour répondre aux différents besoins techniques et analytiques. Voici une présentation des bibliothèques principales et leur rôle.

<details> <summary>`csv`</summary>
La bibliothèque `csv` permet de lire et d’écrire des fichiers CSV, un format commun pour manipuler des données tabulaires.
Nous avons utilisé `csv` pour extraire et traiter les données brutes contenues dans des fichiers au format CSV. Cela est particulièrement utile pour manipuler des ensembles de données simples où une lecture ligne par ligne est nécessaire.
</details>

<details> <summary>`matplotlib.pyplot`</summary>
`matplotlib.pyplot` est utilisée pour produire des graphiques statiques et analyser visuellement les données.
Cette bibliothèque nous permet de visualiser les tendances, les répartitions et les relations dans les données à l’aide de graphiques tels que les histogrammes, les courbes ou les nuages de points.
</details>

<details> <summary>`numpy`</summary>
`numpy` est une bibliothèque puissante pour effectuer des calculs numériques avancés, notamment des opérations matricielles.
Les opérations matricielles et les calculs numériques complexes nécessaires à l’analyse des données sont simplifiés grâce à `numpy`, qui garantit également des performances élevées.
</details>

<details> <summary>`os`</summary>
`os` fournit des fonctions pour interagir avec le système d’exploitation, notamment pour gérer les fichiers et les répertoires.
Nous avons utilisé `os` pour gérer les chemins des fichiers, vérifier l’existence des répertoires, et manipuler les ressources locales du système.
</details>

<details> <summary>`pooch`</summary>
`pooch` facilite le téléchargement et la mise en cache des fichiers nécessaires à l’exécution du projet.
Cette bibliothèque permet de garantir un accès fiable aux données externes en les téléchargeant automatiquement et en les stockant localement pour une réutilisation future.
</details>

<details> <summary>`pandas`</summary>
`pandas` est essentielle pour manipuler et analyser des données tabulaires de manière efficace.
`pandas` est utilisée pour nettoyer, transformer et analyser des ensembles de données complexes, offrant des fonctionnalités avancées comme le traitement des séries temporelles et des jointures de tables.
</details>

<details> <summary>`json`</summary>
`json` est utilisée pour manipuler des données au format JSON, un standard de stockage et d’échange d’informations structurées.
Nous utilisons `json` pour lire et écrire des données structurées, notamment pour gérer les configurations et les résultats intermédiaires dans des fichiers légers.
</details>

<details> <summary>`re`</summary>
La bibliothèque `re` permet de travailler avec des expressions régulières pour manipuler des chaînes de caractères.
Grâce à `re`, nous avons pu extraire des informations spécifiques des chaînes de caractères et nettoyer les données textuelles de manière efficace.
</details>

<details> <summary>`unicodedata`</summary>
`unicodedata` est utilisée pour normaliser les chaînes de caractères Unicode. 
Cette bibliothèque est essentielle pour traiter les caractères spéciaux et garantir la cohérence des chaînes de caractères provenant de différentes sources.
</details>

<details> <summary>`folium`</summary>
`folium` est une bibliothèque dédiée à la création de cartes interactives. 
Nous avons utilisé `folium` pour visualiser les trajets et itinéraires des vélos sur des cartes interactives, permettant une meilleure compréhension géographique des données.
</details>

<details> <summary>`osmnx`</summary>
`osmnx` est utilisée pour le géocodage et l’analyse des réseaux géographiques.
Cette bibliothèque permet de calculer des itinéraires, de géocoder les points d’intérêt et d’extraire des données à partir d’OpenStreetMap pour analyser les réseaux routiers.
</details>

<details> <summary>`functools.lru_cache`</summary>
`functools.lru_cache` est une fonctionnalité de Python pour optimiser les performances.
En mettant en cache les résultats des fonctions fréquemment appelées, `functools.lru_cache` améliore les performances et réduit le temps de calcul pour des opérations répétées.
</details>

## 6.2 Functions

### 6.2.1 Map 

<details> 
  <summary>`colonne(i, w_file)`</summary>

<strong>Description :</strong>  
Cette fonction permet d'extraire une colonne spécifique d'un fichier CSV. Elle prend en entrée un indice `i`, représentant la colonne à extraire, ainsi que le chemin du fichier `w_file`. La fonction ouvre le fichier, parcourt chaque ligne et récupère l'élément situé à la position `i` dans chaque ligne. Le résultat est une liste contenant toutes les valeurs de la colonne souhaitée.

<strong>Paramètres :</strong> 
  <ul>
    <li>`i` (int) : L'indice de la colonne à extraire.</li>
    <li>`w_file` (str) : Le chemin d'accès au fichier CSV.</li>
  </ul>

  <strong>Retourne :</strong> 
  <ul>
    <li>`L` (list) : Une liste contenant les valeurs de la colonne spécifiée.</li>
  </ul>

  <strong>Code de la fonction :</strong>
  
  ```python
  def colonne(i, w_file):
    L=[]
    with open(w_file) as f:
        for line in f:
            x=line.split(",")
            L.append(x[i])
    return L 
  ```
</details>

<details> 
  <summary>`arg(k, i, j, w_file)`</summary>
  
<strong>Description :</strong>  
Cette fonction retourne toutes les valeurs dans la colonne `j` lorsque l'argument dans la colonne `i` est égal à `k`. Elle est utile pour filtrer un ensemble de données en fonction d'une condition donnée dans une colonne spécifique, puis extraire les valeurs correspondantes dans une autre colonne.

<strong>Paramètres :</strong> 
  <ul>
    <li>`k` (str) : La valeur de la colonne `i` que l'on souhaite rechercher.</li>
    <li>`i` (int) : L'indice de la colonne à vérifier.</li>
    <li>`j` (int) : L'indice de la colonne dont les valeurs seront extraites.</li>
    <li>`w_file` (str) : Le chemin d'accès au fichier CSV.</li>
  </ul>

<strong>Retourne :</strong> 
  <ul>
    <li>`L` (list) : Une liste des valeurs de la colonne `j` correspondant à la condition `x[i] == k`.</li>
  </ul>
  
  <strong>Code de la fonction :</strong>

  ```python
  def arg(k,i,j, w_file):
    L=[]
    with open(w_file) as f:
        for line in f:
            x=line.split(",")
            if x[i]==k:
                L.append(x[j])
    return L 
  ```
</details>

<details> 
  <summary>`pd_to_datetime(df, colonne_date)`</summary>

<strong>Description :</strong>    
Cette fonction transforme une colonne de dates dans un DataFrame en un format exploitable. Elle supprime les valeurs manquantes, convertit la colonne de dates en format `datetime` et crée une nouvelle colonne `Date` contenant uniquement la date, en supprimant la colonne initiale des dates.

<strong>Paramètres :</strong> 
  <ul>
    <li>`df` (DataFrame) : Le DataFrame contenant la colonne de dates.</li>
    <li>`colonne_date` (str) : Le nom de la colonne de dates à convertir.</li>
  </ul>

<strong>Retourne :</strong> 
  <ul>
    <li>`df` (DataFrame) : Le DataFrame modifié avec la colonne `Date` et la colonne initiale supprimée.</li>
  </ul>

  <strong>Code de la fonction :</strong>

  ```python
  def pd_to_datetime(df, colonne_date):
    df = df.dropna()
    df[colonne_date] = pd.to_datetime(df[colonne_date])
    df['Date'] = df[colonne_date].dt.date
    df = df.drop(columns=[colonne_date])
    return df
  ```
</details>

<details> 
  <summary>`nettoyer_adresse_normalise(adresse)`</summary>

<strong>Description :</strong>  
Cette fonction nettoie et normalise une adresse en supprimant les numéros au début et en corrigeant les caractères Unicode. Elle utilise des expressions régulières pour enlever les numéros de début d'adresse et applique une normalisation Unicode pour garantir une uniformité des caractères.

<strong>Paramètres :</strong> 
  <ul>
    <li>`adresse` (str) : La chaîne de caractères représentant l'adresse à nettoyer et normaliser.</li>
  </ul>

<strong>Retourne :</strong> 
  <ul>
    <li>str : L'adresse nettoyée et normalisée.</li>
  </ul>

  <strong>Code de la fonction :</strong>

  ```python
  def nettoyer_adresse_normalise(adresse):
    """
    Nettoie et normalise une adresse en supprimant les numéros au début, 
    en normalisant les caractères Unicode.
    
    Paramètre :
    adresse (str) : La chaîne d'adresse à normaliser.
    
    Retourne :
    str : L'adresse nettoyée et normalisée.
    """
    # Tenter de corriger l'encodage si nécessaire
    try:
        # Encode la chaîne en latin1 puis décode en utf-8
        adresse = adresse.encode('latin1').decode('utf-8')
    except (UnicodeEncodeError, UnicodeDecodeError):
        pass  # Ignore les erreurs d'encodage si elles se produisent

    # Supprimer les numéros ou autres formats non pertinents (ex: 057 au début)
    adresse = re.sub(r'^\d+\s*', '', adresse)  # Enlève les numéros au début
    
    # Normalisation des caractères Unicode
    adresse = unicodedata.normalize('NFKD', adresse)
    
    # Retourner l'adresse nettoyée et normalisée
    return adresse  # Enlever les espaces supplémentaires aux extrémités
  ```
</details>

<details> 
  <summary>`gen_carte_trajet(ligne, G, m, index_colonne_départ, index_colonne_arrive, couleur)`</summary>

<strong>Description :</strong>  
Cette fonction génère une carte représentant le trajet entre deux points de départ et d'arrivée. Elle utilise le géocodage pour convertir les noms des stations en coordonnées géographiques, puis calcule l'itinéraire le plus court entre les deux stations sur un graphe de rue. Les itinéraires sont ajoutés à la carte, ainsi que des marqueurs pour les stations de départ et d'arrivée.

<strong>Paramètres :</strong> 
  <ul>
    <li>`ligne` (list) : Une ligne contenant les noms des stations de départ et d'arrivée.</li>
    <li>`G` (Graph) : Le graphe représentant le réseau de rues de la ville.</li>
    <li>`m` (Map) : L'objet de la carte sur lequel le trajet sera tracé.</li>
    <li>`index_colonne_départ` (int) : L'indice de la colonne contenant le nom de la station de départ.</li>
    <li>`index_colonne_arrive` (int) : L'indice de la colonne contenant le nom de la station d'arrivée.</li>
    <li>`couleur` (str) : La couleur de la ligne représentant le trajet.</li>
  </ul>

<strong>Retourne :</strong> 
  <ul>
    <li>`m` (Map) : La carte avec l'itinéraire ajouté.</li>
  </ul>

  <strong>Code de la fonction :</strong>

  ```python
  def gen_carte_trajet(ligne, G, m, index_colonne_départ, index_colonne_arrive,couleur):
    # Essayer de géocoder les stations de départ et d'arrivée
    try:
        origin = ox.geocode(f"{ligne[index_colonne_départ]}, Montpellier, France")  # Première colonne
        destination = ox.geocode(f"{ligne[index_colonne_arrive]}, Montpellier, France")  # Deuxième colonne
        
        # Vérifier si le géocodage a réussi
        if origin is None or destination is None:
            print(f"Erreur de géocodage pour les stations : {ligne[index_colonne_départ]} ou {ligne[index_colonne_arrive]}")
            return m
        
        # Trouver les nœuds les plus proches de l'origine et de la destination
        origin_node = ox.nearest_nodes(G, origin[1], origin[0])  # longitude, latitude
        destination_node = ox.nearest_nodes(G, destination[1], destination[0])  # longitude, latitude

        # Calculer l'itinéraire aller et retour
        route = ox.shortest_path(G, origin_node, destination_node)

        # Fonction pour convertir un itinéraire (liste de nœuds) en liste de coordonnées géographiques
        def route_to_coords(G, route):
            route_coords = []
            for node in route:
                point = (G.nodes[node]['y'], G.nodes[node]['x'])  # latitude, longitude
                route_coords.append(point)
            return route_coords

        # Obtenir les coordonnées pour l'itinéraire
        route_coords = route_to_coords(G, route)

        # Ajouter l'itinéraire aller (en rouge) à la carte
        folium.PolyLine(locations=route_coords, color=couleur, weight=5, opacity=0.75).add_to(m)

        # Ajouter des marqueurs pour l'origine et la destination
        départ_lat, départ_lon = route_coords[0]
        arr_lat, arr_lon = route_coords[-1]  # Utiliser le dernier point pour l'arrivée
        folium.Marker(location=[départ_lat, départ_lon], popup=f"{ligne[index_colonne_départ]},Départ").add_to(m)
        folium.Marker(location=[arr_lat, arr_lon], popup=f"{ligne[index_colonne_arrive]},arrivé").add_to(m)

    except Exception as e:
        print(f"Une erreur est survenue : {e}")
    
    return m
  ```
</details>

<details> 
  <summary>`coordonne(station)`</summary>

<strong>Description :</strong>  
Cette fonction permet de géocoder le nom d'une station pour obtenir ses coordonnées géographiques (latitude et longitude). Elle utilise la bibliothèque `osmnx` pour rechercher l'emplacement correspondant à la station spécifiée dans la ville de Montpellier, France.

<strong>Paramètres :</strong> 
  <ul>
    <li>`station` (str) : Le nom de la station à géocoder.</li>
  </ul>

<strong>Retourne :</strong> 
  <ul>
    <li>`latitude` (float) : La latitude de la station.</li>
    <li>`longitude` (float) : La longitude de la station.</li>
  </ul>

  <strong>Code de la fonction :</strong>

  ```python
  def coordonne(station):
    try:
        # Recherche de l'emplacement en utilisant osmnx
        location = ox.geocode(f"{station}, Montpellier, France")
        return location[0], location[1]
    except Exception as e:
        print(f"Erreur pour la station {station}: {e}")
        return None, None
  ```
</details>

### 6.2.2 Préduction

### 6.2.3 Video 

## 6.3 Class

[HERE IS A TEXT ABOUT CLASS]

# 7. Testing

# 8. Performance Analysis

# 9. User Guide

# 10. Travaux futurs

Bien que ce projet ait permis de poser les bases d'une analyse approfondie du trafic cycliste à Montpellier, plusieurs pistes de développement restent ouvertes pour enrichir et étendre cette étude. Voici quelques axes possibles pour les travaux futurs :

- **Amélioration de la précision des prédictions** : Une direction importante consiste à améliorer les modèles de prédiction du trafic en utilisant des techniques d'apprentissage automatique avancées, telles que les réseaux de neurones ou l'apprentissage par renforcement. Cela pourrait permettre de mieux anticiper les variations du trafic en fonction de facteurs contextuels tels que la météo ou les événements locaux.
  
- **Intégration de données en temps réel** : L'ajout de données en temps réel, comme les informations sur la circulation, les conditions météorologiques ou même les perturbations du réseau de transport (par exemple, les travaux de construction), pourrait rendre les prévisions plus dynamiques et réactives aux conditions changeantes.

- **Analyse comparative entre différentes villes** : Une extension naturelle de ce projet serait d'appliquer la même méthodologie à d'autres villes. Cela permettrait de comparer les tendances du trafic cycliste entre différentes régions et d'identifier des modèles communs ou des différences spécifiques aux contextes locaux.

- **Développement d'une carte de trafic basée sur les événements** : Un projet complémentaire consisterait à intégrer des informations sur les événements urbains (concerts, manifestations, événements sportifs, etc.) dans le système de prédiction du trafic. Cela permettrait de prédire des hausses de trafic liées à ces événements, offrant ainsi des informations précieuses aux utilisateurs.

- **Optimisation de l'interface utilisateur** : L'interface du site Web pourrait être améliorée en offrant davantage de fonctionnalités interactives, telles que la planification d'itinéraires en fonction des prévisions de trafic ou des suggestions d'itinéraires alternatifs pour éviter les zones congestionnées.

- **Étude de l'impact environnemental** : Une analyse plus poussée pourrait être menée sur l'impact du trafic cycliste sur la qualité de l'air et la pollution. Cette étude pourrait inclure la corrélation entre le trafic vélo et les niveaux de pollution pour proposer des solutions visant à réduire les émissions dans les zones les plus fréquentées.

Ces directions futures ne sont pas exhaustives et peuvent évoluer en fonction des avancées technologiques et des retours d'expérience des utilisateurs. Elles témoignent de l'ambition du projet d'offrir des solutions durables et efficaces pour la gestion du trafic cycliste en milieu urbain.
