---
title: Documentation
format: html
---

# 1. Introduction

**CycleVision3** est un projet visant à analyser et prédire les flux cyclistes à Montpellier en exploitant des données provenant de trois sources principales :  

- **VéloMagg** : Historique des trajets effectués avec les vélos en libre-service.  
- **Capteurs cyclistes et piétons** : Mesures de flux réalisées aux points stratégiques de la ville.  
- **OpenStreetMap (OSM)** : Données géographiques ouvertes pour la cartographie et la contextualisation des trajets.  

L’objectif principal est de produire des **visualisations interactives**, incluant une **carte prédictive** des flux cyclistes, accessible via un site web. Ces outils permettront d’identifier les tendances de mobilité et d’offrir des solutions pratiques pour améliorer les infrastructures urbaines et promouvoir l’usage du vélo.  

En associant innovation technologique et utilité sociale, **CycleVision3** se veut un modèle reproductible pour d’autres villes, contribuant ainsi à une mobilité urbaine durable et intelligente.

# 2. Project Setup



- **Traffic Analysis**: Analyze bike traffic data to identify trends, peak times, and popular routes.
- **Interactive Visualization**: Create dynamic maps that visualize traffic flows across Montpellier.
- **Web Integration**: Develop a user-friendly website to showcase data insights and traffic maps.

## 2.1 Structure du projet [WILL BE FINISHED AT THE END]

La structure du projet est organisée de manière à assurer une séparation claire des différentes composantes nécessaires au bon déroulement de l'analyse et à la création des visualisations. Voici un aperçu de l'organisation des fichiers et répertoires :

<details> <summary>Voir le structure</summary>

```
main/                                           # Répertoire principal du projet
├── .github/workflows/                          # 
├── analyse_donnee/                             # 
├── cache/                                      # Fichiers temporaires
├── data/                                       # Stockage des données des 3 jeux de données
│   ├── extracted/                              # 
│   ├── CoursesVelomagg.csv                     # Données sur les trajets de vélos en libre-service
│   ├── video.csv                               # 
│   └── video_avec_coord.csv                    # 
├── docs/                                       # Répertoire des fichiers du site web
│   ├── .gitignore                              # Liste des fichiers à ignorer par Git
│   ├──                                         # 
│   ├──                                         # 
│   ├──                                         # 
│   └── styles.css                              # Fichier CSS pour la mise en forme du site
├── images/                                     # Images utilisées dans le projet
├── map/                                        # 
├── roadmap/                                    # Fichier README.md avec la description .......
├── slide/                                      # 
├── src/                                        # Répertoire contenant le code source
│   ├── __init__.py                             # Initialisation du package
│   ├── donnée.py                               # Contient les fonctions liées aux données
│   └── fonctions_basedonnees.py                # Fonctions pour les opérations sur la base de données
├── vidéo/                                      # 
├── visualisation/                              # Résultats des visualisations
├──.DS_Store                                    # 
├──
├──
├── .Rhistory                                   # Fichier d'historique R
├── .gitignore                                  # Fichier Git ignore
├── README.md                                   # Documentation principale du projet
└── requirements.txt                            # Dépendances du projet
```
</details>

## 2.2 Installation et dépendances

Avant de commencer le développement et l'analyse des données, il est nécessaire de configurer l'environnement de travail en installant toutes les dépendances nécessaires. Voici les étapes à suivre pour préparer le projet :

1. **Cloner le dépôt Git** : 
   La première étape consiste à cloner le dépôt du projet depuis GitHub à l'aide de la commande suivante :
   ```bash
   git clone https://github.com/mscaia/PROJ_HAX712X.git
   ```
   
2. **Installer les dépendances** :
   Le projet repose sur plusieurs bibliothèques Python pour l'analyse des données, la création de visualisations interactives et la génération du site web. Pour installer ces bibliothèques, utilisez la commande suivante :
   ```bash
   pip install -r requirements.txt
   ```

3. **Vérification de l'environnement** :
   Après l'installation des dépendances, vous pouvez vérifier que tout fonctionne correctement en exécutant un script de test ou en accédant au site web via `quarto` :
   ```bash
   quarto preview
   ```
   Cela lancera un serveur local où vous pourrez visualiser le site et tester les fonctionnalités interactives.

## 2.3 Configuration des fichiers [**Sphinx** - CHEKED]

Le projet est configuré de manière à utiliser **Quarto** pour la création du site web interactif et [**Sphinx** pour la documentation générée automatiquement]. Le fichier `_quarto.yml` contient les paramètres de configuration du projet Quarto, tandis que le fichier `index.qmd` contient le contenu principal de la page d'accueil du site.

## 2.4 Base de données et traitement des données [SHOULD BE RECHEKED AFTER]

Le répertoire `data/` contient les trois principaux jeux de données utilisés dans ce projet :

- **CoursesVelomagg.csv** : Contient les informations sur les trajets effectués avec les vélos en libre-service.
- **video.csv** : 
- **video_avec_coord.csv** : 

Le traitement de ces données est effectué dans le répertoire `src/`, où les scripts comme `donnée.py` et `fonctions_basedonnees.py` contiennent les fonctions nécessaires pour charger, nettoyer et manipuler les données.

## 2.5 Exécution du projet

Une fois l'environnement configuré et les données traitées, il suffit de lancer le script principal pour générer les visualisations interactives et les cartes prédictives du trafic cycliste. Le projet utilise **Folium** pour la visualisation sur carte et **Matplotlib** pour les graphiques statistiques.

Le répertoire `visualisation/` contient les scripts et sorties graphiques générées au fur et à mesure du traitement des données, notamment les cartes des trajets cyclistes et les prévisions de trafic.

## 2.6 Documentation et tests [FINISH AND RECHECKED]

Pour garantir la qualité du code et la conformité aux exigences du projet, des tests unitaires sont fournis pour vérifier le bon fonctionnement des différentes fonctionnalités. La documentation technique est générée via **Sphinx** et **Quarto**, offrant ainsi une vue d'ensemble sur la structure du code, les API disponibles et la méthodologie utilisée.

# 3. Description des Données [RECHECK ABOOUT THE DATASETS WITH GUYS]

L’analyse repose sur l’exploitation de trois ensembles de données principaux, chacun jouant un rôle clé dans l’étude des trajets cyclistes à Montpellier :

- **Données des trajets en vélos en libre-service (VéloMagg)** : Ces données fournissent des informations détaillées sur l’utilisation des vélos en libre-service dans la métropole de Montpellier. Elles constituent une base essentielle pour analyser les tendances et les flux de déplacements cyclistes.  
  *Source*: [Trajets à vélos en libre-service](https://data.montpellier3m.fr/dataset/courses-des-velos-velomagg-de-montpellier-mediterranee-metropole)  

- **Données de comptage cycliste et piéton** : Ces données, collectées par des capteurs placés à divers emplacements stratégiques, permettent de mesurer les flux de cyclistes et de piétons dans la ville. Elles enrichissent l'analyse en fournissant une perspective quantitative sur la fréquentation des infrastructures.  
  *Source*: [Comptages vélo et piéton issus des compteurs de vélo](https://data.montpellier3m.fr/dataset/comptages-velo-et-pieton-issus-des-compteurs-de-velo)  

- **Données géographiques OpenStreetMap (OSM)** : Ces données géographiques offrent une cartographie précise des rues, pistes cyclables et points d’intérêt de la ville. Elles sont cruciales pour la visualisation et la spatialisation des trajets étudiés.  
  *Source*: [OpenStreetMap](https://www.openstreetmap.org/#map=6/46.45/2.21)  

Ces trois ensembles de données, complémentaires par nature, permettent une analyse approfondie et multidimensionnelle du trafic cycliste à Montpellier.

# 4. Project Workflow [CHECK WHOLE SECTION - ENG]

## 4.1 Data Cleaning and Preprocessing

### Tasks:
- **Data Integrity**: Clean the data by removing anomalies, handling missing values, and ensuring consistent formatting.
- **Date and Address Formatting**: Convert date fields for temporal analysis and standardize address formatting to enable efficient location-based processing.

## 4.2 Traffic Visualization and Mapping

### Tasks:
- **Map Generation**: Use the Folium library to generate an interactive map of Montpellier, displaying bike stations, routes, and traffic density.
- **Route Calculation**: Utilize OSMnx to calculate the routes between bike stations, highlighting the most popular paths and enabling traffic analysis over time.

## 4.3 Website Development

The final deliverable will be an interactive website, presenting all the data visualizations and analytical insights. The website will be developed using Quarto for easy integration of analysis and visualizations into an accessible, interactive interface.

# 5. Pipeline de Traitement des Données

Le traitement des données constitue une étape clé du projet **CycleVision3**, permettant de transformer les données brutes en informations exploitables pour l’analyse et la modélisation du trafic cycliste. La méthodologie suit une structure modulaire pour garantir flexibilité et robustesse. Voici les principales étapes de notre pipeline :

## 5.1 Acquisition des données

Les ensembles de données ont été collectés auprès de sources officielles :

- **Données VéloMagg** : Historique des trajets effectués via le système de vélos en libre-service.
- **Comptages cyclistes et piétons** : Relevés par des capteurs aux points stratégiques de la ville.
- **Données OpenStreetMap (OSM)** : Informations géographiques pour la cartographie.

Ces fichiers ont été extraits au format `.csv` et `.json` et chargés à l’aide de la bibliothèque `pandas`. Une validation initiale a été effectuée pour vérifier l’intégrité des fichiers (taille, colonnes attendues, etc.).

## 5.2 Prétraitement
L’étape de prétraitement vise à préparer les données pour l’analyse. Les principales opérations incluent :

  <ul>
    <li>**Nettoyage des données** : Suppression des valeurs manquantes et des doublons.</li>
    <li>**Transformation des variables** : Normalisation des données temporelles pour harmoniser les différents jeux de données.</li>
    <li>**Filtrage géographique** : Limitation aux trajets effectués dans Montpellier.</li>
  </ul>

## 5.3 Analyse exploratoire
Une exploration préliminaire a permis de dégager des tendances :

  <ul>
    <li>Analyse temporelle des flux de cyclistes (par heure, jour, mois).</li>
    <li>Cartographie des zones à fort trafic à l’aide de `Folium`.</li>
    <li>Visualisation des variations de densité de trafic.</li>
  </ul>

Ces étapes ont permis d’orienter les choix algorithmiques pour la prédiction.

## 5.4 Modélisation
Un algorithme de prédiction du trafic a été conçu en se basant sur des variables clés :

  <ul>
    <li>Données historiques des trajets.</li>
    <li>Localisation et fréquence des trajets.</li>
    <li>Influence potentielle des heures de pointe et des jours fériés.</li>
  </ul>

L’implémentation repose sur des modèles de régression supervisée. Une attention particulière a été portée à la validation croisée pour assurer la fiabilité des prédictions.

## 5.5 Visualisation
Les résultats sont intégrés dans des visualisations interactives :

  <ul>
    <li>Cartes dynamiques illustrant les trajets par zones.</li>
    <li>Tableau de bord synthétisant les flux par période.</li>
  </ul>

L’ensemble de ces visualisations est hébergé sur le site web interactif du projet.

## 5.6 Intégration
Le pipeline est conçu pour être adaptable :

  <ul>
    <li>**Portabilité** : Facilité d’application à d’autres villes disposant de systèmes similaires.</li>
    <li>**Scalabilité** : Support de nouveaux ensembles de données (ex. pollution ou météo).</li>
  </ul>

## Perspectives
Pour enrichir le pipeline, des pistes futures incluent l’intégration de données en temps réel et l’optimisation de la performance pour traiter des volumes massifs de données.

# 6. Technical Documentation

## 6.1 Bibliothèques utilisées

Dans le cadre de ce projet, plusieurs bibliothèques ont été utilisées pour répondre aux différents besoins techniques et analytiques. Voici une présentation des bibliothèques principales et leur rôle.

<details> <summary>`csv`</summary>
La bibliothèque `csv` permet de lire et d’écrire des fichiers CSV, un format commun pour manipuler des données tabulaires.
Nous avons utilisé `csv` pour extraire et traiter les données brutes contenues dans des fichiers au format CSV. Cela est particulièrement utile pour manipuler des ensembles de données simples où une lecture ligne par ligne est nécessaire.
</details>

<details> <summary>`matplotlib.pyplot`</summary>
`matplotlib.pyplot` est utilisée pour produire des graphiques statiques et des visualisations animées.
Dans ce projet, elle permet de créer des graphiques dynamiques illustrant les trajectoires cyclistes et d’exporter ces visualisations sous forme de vidéos à l'aide des modules `FuncAnimation` et `FFMpegWriter`.
</details>

<details> <summary>`matplotlib.animation.FuncAnimation` et `FFMpegWriter`</summary>
Ces modules de la bibliothèque `matplotlib.animation` permettent de créer des animations et d'exporter celles-ci sous forme de fichiers vidéo.
Dans le projet, ils sont utilisés pour générer des animations illustrant les variations temporelles du trafic cycliste et les enregistrer sous un format visuel accessible.
</details>

<details> <summary>`numpy`</summary>
`numpy` est une bibliothèque puissante pour effectuer des calculs numériques avancés, notamment des opérations matricielles.
Les opérations matricielles et les calculs numériques complexes nécessaires à l’analyse des données sont simplifiés grâce à `numpy`, qui garantit également des performances élevées.
</details>

<details> <summary>`os`</summary>
`os` fournit des fonctions pour interagir avec le système d’exploitation, notamment pour gérer les fichiers et les répertoires.
Nous avons utilisé `os` pour gérer les chemins des fichiers, vérifier l’existence des répertoires, et manipuler les ressources locales du système.
</details>

<details> <summary>`pooch`</summary>
`pooch` facilite le téléchargement et la mise en cache des fichiers nécessaires à l’exécution du projet.
Cette bibliothèque permet de garantir un accès fiable aux données externes en les téléchargeant automatiquement et en les stockant localement pour une réutilisation future.
</details>

<details> <summary>`pandas`</summary>
`pandas` est essentielle pour manipuler et analyser des données tabulaires de manière efficace.
`pandas` est utilisée pour nettoyer, transformer et analyser des ensembles de données complexes, offrant des fonctionnalités avancées comme le traitement des séries temporelles et des jointures de tables.
</details>

<details> <summary>`json`</summary>
`json` est utilisée pour manipuler des données au format JSON, un standard de stockage et d’échange d’informations structurées.
Nous utilisons `json` pour lire et écrire des données structurées, notamment pour gérer les configurations et les résultats intermédiaires dans des fichiers légers.
</details>

<details> <summary>`re`</summary>
La bibliothèque `re` permet de travailler avec des expressions régulières pour manipuler des chaînes de caractères.
Grâce à `re`, nous avons pu extraire des informations spécifiques des chaînes de caractères et nettoyer les données textuelles de manière efficace.
</details>

<details> <summary>`unicodedata`</summary>
`unicodedata` est utilisée pour normaliser les chaînes de caractères Unicode. 
Cette bibliothèque est essentielle pour traiter les caractères spéciaux et garantir la cohérence des chaînes de caractères provenant de différentes sources.
</details>

<details> <summary>`folium`</summary>
`folium` est une bibliothèque dédiée à la création de cartes interactives. 
Nous avons utilisé `folium` pour visualiser les trajets et itinéraires des vélos sur des cartes interactives, permettant une meilleure compréhension géographique des données.
</details>

<details> <summary>`osmnx`</summary>
`osmnx` est utilisée pour le géocodage et l’analyse des réseaux géographiques.
Cette bibliothèque permet d’extraire des données géographiques d’OpenStreetMap, de construire des graphes routiers, et d’analyser les itinéraires et les trajets cyclistes dans le cadre de ce projet.
</details>

<details> <summary>`functools.lru_cache`</summary>
`functools.lru_cache` est une fonctionnalité de Python pour optimiser les performances.
En mettant en cache les résultats des fonctions fréquemment appelées, `functools.lru_cache` améliore les performances et réduit le temps de calcul pour des opérations répétées.
</details>

<details> <summary>`networkx`</summary>
`networkx` est une bibliothèque dédiée à la création, la manipulation et l'analyse de graphes complexes.
Dans ce projet, elle est utilisée pour représenter et étudier les réseaux cyclistes, notamment pour visualiser les trajets et calculer les chemins les plus courts entre les nœuds.
</details>

<details> <summary>`concurrent.futures.ThreadPoolExecutor`</summary>
`ThreadPoolExecutor` est une fonctionnalité du module standard `concurrent.futures` pour exécuter des tâches en parallèle.
Elle est utilisée pour optimiser le traitement des données et accélérer le rendu des animations dans le cadre de ce projet.
</details>

<details> <summary>`datetime`</summary>
`datetime` est un module intégré pour manipuler les dates et les heures.
Dans le projet, il est employé pour traiter les données temporelles des trajets cyclistes et synchroniser les animations avec les horodatages.
</details>

## 6.2 Functions

### 6.2.1 Map 

<details> 
  <summary>`colonne(i, w_file)`</summary>

<strong>Description :</strong>  
Cette fonction permet d'extraire une colonne spécifique d'un fichier CSV. Elle prend en entrée un indice `i`, représentant la colonne à extraire, ainsi que le chemin du fichier `w_file`. La fonction ouvre le fichier, parcourt chaque ligne et récupère l'élément situé à la position `i` dans chaque ligne. Le résultat est une liste contenant toutes les valeurs de la colonne souhaitée.

<strong>Paramètres :</strong> 
  <ul>
    <li>`i` (int) : L'indice de la colonne à extraire.</li>
    <li>`w_file` (str) : Le chemin d'accès au fichier CSV.</li>
  </ul>

  <strong>Retourne :</strong> 
  <ul>
    <li>`L` (list) : Une liste contenant les valeurs de la colonne spécifiée.</li>
  </ul>

  <strong>Code de la fonction :</strong>
  
  ```python
  def colonne(i, w_file):
    L=[]
    with open(w_file) as f:
        for line in f:
            x=line.split(",")
            L.append(x[i])
    return L 
  ```
</details>

<details> 
  <summary>`arg(k, i, j, w_file)`</summary>
  
<strong>Description :</strong>  
Cette fonction retourne toutes les valeurs dans la colonne `j` lorsque l'argument dans la colonne `i` est égal à `k`. Elle est utile pour filtrer un ensemble de données en fonction d'une condition donnée dans une colonne spécifique, puis extraire les valeurs correspondantes dans une autre colonne.

<strong>Paramètres :</strong> 
  <ul>
    <li>`k` (str) : La valeur de la colonne `i` que l'on souhaite rechercher.</li>
    <li>`i` (int) : L'indice de la colonne à vérifier.</li>
    <li>`j` (int) : L'indice de la colonne dont les valeurs seront extraites.</li>
    <li>`w_file` (str) : Le chemin d'accès au fichier CSV.</li>
  </ul>

<strong>Retourne :</strong> 
  <ul>
    <li>`L` (list) : Une liste des valeurs de la colonne `j` correspondant à la condition `x[i] == k`.</li>
  </ul>
  
  <strong>Code de la fonction :</strong>

  ```python
  def arg(k,i,j, w_file):
    L=[]
    with open(w_file) as f:
        for line in f:
            x=line.split(",")
            if x[i]==k:
                L.append(x[j])
    return L 
  ```
</details>

<details> 
  <summary>`pd_to_datetime(df, colonne_date)`</summary>

<strong>Description :</strong>    
Cette fonction transforme une colonne de dates dans un DataFrame en un format exploitable. Elle supprime les valeurs manquantes, convertit la colonne de dates en format `datetime` et crée une nouvelle colonne `Date` contenant uniquement la date, en supprimant la colonne initiale des dates.

<strong>Paramètres :</strong> 
  <ul>
    <li>`df` (DataFrame) : Le DataFrame contenant la colonne de dates.</li>
    <li>`colonne_date` (str) : Le nom de la colonne de dates à convertir.</li>
  </ul>

<strong>Retourne :</strong> 
  <ul>
    <li>`df` (DataFrame) : Le DataFrame modifié avec la colonne `Date` et la colonne initiale supprimée.</li>
  </ul>

  <strong>Code de la fonction :</strong>

  ```python
  def pd_to_datetime(df, colonne_date):
    df = df.dropna()
    df[colonne_date] = pd.to_datetime(df[colonne_date])
    df['Date'] = df[colonne_date].dt.date
    df = df.drop(columns=[colonne_date])
    return df
  ```
</details>

<details> 
  <summary>`nettoyer_adresse_normalise(adresse)`</summary>

<strong>Description :</strong>  
Cette fonction nettoie et normalise une adresse en supprimant les numéros au début et en corrigeant les caractères Unicode. Elle utilise des expressions régulières pour enlever les numéros de début d'adresse et applique une normalisation Unicode pour garantir une uniformité des caractères.

<strong>Paramètres :</strong> 
  <ul>
    <li>`adresse` (str) : La chaîne de caractères représentant l'adresse à nettoyer et normaliser.</li>
  </ul>

<strong>Retourne :</strong> 
  <ul>
    <li>str : L'adresse nettoyée et normalisée.</li>
  </ul>

  <strong>Code de la fonction :</strong>

  ```python
  def nettoyer_adresse_normalise(adresse):
    """
    Nettoie et normalise une adresse en supprimant les numéros au début, 
    en normalisant les caractères Unicode.
    
    Paramètre :
    adresse (str) : La chaîne d'adresse à normaliser.
    
    Retourne :
    str : L'adresse nettoyée et normalisée.
    """
    # Tenter de corriger l'encodage si nécessaire
    try:
        # Encode la chaîne en latin1 puis décode en utf-8
        adresse = adresse.encode('latin1').decode('utf-8')
    except (UnicodeEncodeError, UnicodeDecodeError):
        pass  # Ignore les erreurs d'encodage si elles se produisent

    # Supprimer les numéros ou autres formats non pertinents (ex: 057 au début)
    adresse = re.sub(r'^\d+\s*', '', adresse)  # Enlève les numéros au début
    
    # Normalisation des caractères Unicode
    adresse = unicodedata.normalize('NFKD', adresse)
    
    # Retourner l'adresse nettoyée et normalisée
    return adresse  # Enlever les espaces supplémentaires aux extrémités
  ```
</details>

<details> 
  <summary>`gen_carte_trajet(ligne, G, m, index_colonne_départ, index_colonne_arrive, couleur)`</summary>

<strong>Description :</strong>  
Cette fonction génère une carte représentant le trajet entre deux points de départ et d'arrivée. Elle utilise le géocodage pour convertir les noms des stations en coordonnées géographiques, puis calcule l'itinéraire le plus court entre les deux stations sur un graphe de rue. Les itinéraires sont ajoutés à la carte, ainsi que des marqueurs pour les stations de départ et d'arrivée.

<strong>Paramètres :</strong> 
  <ul>
    <li>`ligne` (list) : Une ligne contenant les noms des stations de départ et d'arrivée.</li>
    <li>`G` (Graph) : Le graphe représentant le réseau de rues de la ville.</li>
    <li>`m` (Map) : L'objet de la carte sur lequel le trajet sera tracé.</li>
    <li>`index_colonne_départ` (int) : L'indice de la colonne contenant le nom de la station de départ.</li>
    <li>`index_colonne_arrive` (int) : L'indice de la colonne contenant le nom de la station d'arrivée.</li>
    <li>`couleur` (str) : La couleur de la ligne représentant le trajet.</li>
  </ul>

<strong>Retourne :</strong> 
  <ul>
    <li>`m` (Map) : La carte avec l'itinéraire ajouté.</li>
  </ul>

  <strong>Code de la fonction :</strong>

  ```python
  def gen_carte_trajet(ligne, G, m, index_colonne_départ, index_colonne_arrive,couleur):
    # Essayer de géocoder les stations de départ et d'arrivée
    try:
        origin = ox.geocode(f"{ligne[index_colonne_départ]}, Montpellier, France")  # Première colonne
        destination = ox.geocode(f"{ligne[index_colonne_arrive]}, Montpellier, France")  # Deuxième colonne
        
        # Vérifier si le géocodage a réussi
        if origin is None or destination is None:
            print(f"Erreur de géocodage pour les stations : {ligne[index_colonne_départ]} ou {ligne[index_colonne_arrive]}")
            return m
        
        # Trouver les nœuds les plus proches de l'origine et de la destination
        origin_node = ox.nearest_nodes(G, origin[1], origin[0])  # longitude, latitude
        destination_node = ox.nearest_nodes(G, destination[1], destination[0])  # longitude, latitude

        # Calculer l'itinéraire aller et retour
        route = ox.shortest_path(G, origin_node, destination_node)

        # Fonction pour convertir un itinéraire (liste de nœuds) en liste de coordonnées géographiques
        def route_to_coords(G, route):
            route_coords = []
            for node in route:
                point = (G.nodes[node]['y'], G.nodes[node]['x'])  # latitude, longitude
                route_coords.append(point)
            return route_coords

        # Obtenir les coordonnées pour l'itinéraire
        route_coords = route_to_coords(G, route)

        # Ajouter l'itinéraire aller (en rouge) à la carte
        folium.PolyLine(locations=route_coords, color=couleur, weight=5, opacity=0.75).add_to(m)

        # Ajouter des marqueurs pour l'origine et la destination
        départ_lat, départ_lon = route_coords[0]
        arr_lat, arr_lon = route_coords[-1]  # Utiliser le dernier point pour l'arrivée
        folium.Marker(location=[départ_lat, départ_lon], popup=f"{ligne[index_colonne_départ]},Départ").add_to(m)
        folium.Marker(location=[arr_lat, arr_lon], popup=f"{ligne[index_colonne_arrive]},arrivé").add_to(m)

    except Exception as e:
        print(f"Une erreur est survenue : {e}")
    
    return m
  ```
</details>

<details> 
  <summary>`coordonne(station)`</summary>

<strong>Description :</strong>  
Cette fonction permet de géocoder le nom d'une station pour obtenir ses coordonnées géographiques (latitude et longitude). Elle utilise la bibliothèque `osmnx` pour rechercher l'emplacement correspondant à la station spécifiée dans la ville de Montpellier, France.

<strong>Paramètres :</strong> 
  <ul>
    <li>`station` (str) : Le nom de la station à géocoder.</li>
  </ul>

<strong>Retourne :</strong> 
  <ul>
    <li>`latitude` (float) : La latitude de la station.</li>
    <li>`longitude` (float) : La longitude de la station.</li>
  </ul>

  <strong>Code de la fonction :</strong>

  ```python
  def coordonne(station):
    try:
        # Recherche de l'emplacement en utilisant osmnx
        location = ox.geocode(f"{station}, Montpellier, France")
        return location[0], location[1]
    except Exception as e:
        print(f"Erreur pour la station {station}: {e}")
        return None, None
  ```
</details>

### 6.2.2 Préduction [DOCUM FROM CA - PUT HERE]

### 6.2.3 Video [VIDEO FUNCTION]

## 6.3 Class

[HERE IS A TEXT ABOUT CLASS]

# 7. Testing

[tests]

# 8. Performance Analysis

[analys testing here]

# 9. User Guide



# 10. Travaux futurs

Ce projet constitue une première étape dans l'analyse du trafic cycliste à Montpellier. Plusieurs pistes peuvent être explorées pour enrichir et élargir cette étude :

- **Renforcement des modèles de prédiction** : Une amélioration des modèles actuels pourrait être réalisée en intégrant des données supplémentaires, comme les conditions météorologiques ou les événements locaux, afin d'obtenir des prévisions plus précises.

- **Ajout de données en temps réel** : L'intégration de données en temps réel, telles que les flux de trafic ou les perturbations temporaires, pourrait rendre les analyses plus dynamiques et adaptées à l'évolution des conditions.

- **Application à d'autres contextes urbains** : Étendre cette méthodologie à d'autres villes permettrait de comparer les tendances cyclistes et d'identifier des spécificités locales ou des modèles universels.

- **Amélioration de l'expérience utilisateur** : Le site Web pourrait être enrichi de fonctionnalités interactives supplémentaires, telles que la visualisation des prévisions de trafic ou des recommandations d'itinéraires alternatifs.

Ces propositions, bien qu'ambitieuses, restent accessibles et permettent d'assurer la pertinence et la durabilité du projet tout en renforçant son impact pour les usagers et les décideurs locaux.