[
  {
    "objectID": "projet3.html",
    "href": "projet3.html",
    "title": "Projet 3",
    "section": "",
    "text": "Et là, la vidéo que nous avons produit."
  },
  {
    "objectID": "projet1.html",
    "href": "projet1.html",
    "title": "Projet 1",
    "section": "",
    "text": "Ci-dessous, quelques éléments d’analyse des données. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLe graphique ci-dessus montre […]"
  },
  {
    "objectID": "docu.html",
    "href": "docu.html",
    "title": "Documentation",
    "section": "",
    "text": "CycleVision3 est un projet visant à analyser et prédire les flux cyclistes à Montpellier en exploitant des données provenant de trois sources principales :\n\nVéloMagg : Historique des trajets effectués avec les vélos en libre-service.\n\nCapteurs cyclistes et piétons : Mesures de flux réalisées aux points stratégiques de la ville.\n\nOpenStreetMap (OSM) : Données géographiques ouvertes pour la cartographie et la contextualisation des trajets.\n\nL’objectif principal est de produire des visualisations interactives, incluant une carte prédictive des flux cyclistes, accessible via un site web. Ces outils permettront d’identifier les tendances de mobilité et d’offrir des solutions pratiques pour améliorer les infrastructures urbaines et promouvoir l’usage du vélo.\nEn associant innovation technologique et utilité sociale, CycleVision3 se veut un modèle reproductible pour d’autres villes, contribuant ainsi à une mobilité urbaine durable et intelligente."
  },
  {
    "objectID": "docu.html#data-cleaning-and-preprocessing",
    "href": "docu.html#data-cleaning-and-preprocessing",
    "title": "Documentation",
    "section": "4.1 Data Cleaning and Preprocessing",
    "text": "4.1 Data Cleaning and Preprocessing\n\nTasks:\n\nData Integrity: Clean the data by removing anomalies, handling missing values, and ensuring consistent formatting.\nDate and Address Formatting: Convert date fields for temporal analysis and standardize address formatting to enable efficient location-based processing."
  },
  {
    "objectID": "docu.html#traffic-visualization-and-mapping",
    "href": "docu.html#traffic-visualization-and-mapping",
    "title": "Documentation",
    "section": "4.2 Traffic Visualization and Mapping",
    "text": "4.2 Traffic Visualization and Mapping\n\nTasks:\n\nMap Generation: Use the Folium library to generate an interactive map of Montpellier, displaying bike stations, routes, and traffic density.\nRoute Calculation: Utilize OSMnx to calculate the routes between bike stations, highlighting the most popular paths and enabling traffic analysis over time."
  },
  {
    "objectID": "docu.html#website-development",
    "href": "docu.html#website-development",
    "title": "Documentation",
    "section": "4.3 Website Development",
    "text": "4.3 Website Development\nThe final deliverable will be an interactive website, presenting all the data visualizations and analytical insights. The website will be developed using Quarto for easy integration of analysis and visualizations into an accessible, interactive interface."
  },
  {
    "objectID": "docu.html#additional-features",
    "href": "docu.html#additional-features",
    "title": "Documentation",
    "section": "6.1 Additional Features",
    "text": "6.1 Additional Features\n\nPollution and Traffic Correlation: Investigate correlations between bike traffic patterns and air quality metrics.\nReal-Time Traffic Analysis: Integrate live data feeds for real-time updates, route suggestions, and traffic monitoring.\nAlternative Routes: Offer optimized routes with lower traffic density, based on real-time or historical traffic data."
  },
  {
    "objectID": "auteurs.html",
    "href": "auteurs.html",
    "title": "Contributeurs",
    "section": "",
    "text": "Email\nGitHub Profile\n\n\n\n\n\nEmail\nGitHub Profile\n\n\n\n\n\nEmail\nGitHub Profile\n\n\n\n\n\nEmail\nGitHub Profile"
  },
  {
    "objectID": "auteurs.html#authors",
    "href": "auteurs.html#authors",
    "title": "Contributeurs",
    "section": "",
    "text": "ARMAND Charlotte\nMail\nGit\nCONDAMY Fabian\nMail\nGit\nSCAIA Matteo\nMail\nGit\nSTETSUN Kateryna\nMail\nGit"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visualisation des trajets en vélo dans la ville de Montpellier.",
    "section": "",
    "text": "CycleVision3 est un projet qui vise à décrire le trafic cycliste à Montpellier à travers l’analyse des quelques jeux de données suivants :\n\nTrajets à vélos en libre-service\nComptages vélo et piéton issus des compteurs de vélo\nOpenStreetMap\n\nIl s’agit principalement d’un projet de visualisation, avec des cartes (dont certaines sont interactives) et une vidéo. Les principaux projets sont disponibles dans l’onglet éponyme."
  },
  {
    "objectID": "index.html#a-propos",
    "href": "index.html#a-propos",
    "title": "Visualisation des trajets en vélo dans la ville de Montpellier.",
    "section": "",
    "text": "CycleVision3 est un projet qui vise à décrire le trafic cycliste à Montpellier à travers l’analyse des quelques jeux de données suivants :\n\nTrajets à vélos en libre-service\nComptages vélo et piéton issus des compteurs de vélo\nOpenStreetMap\n\nIl s’agit principalement d’un projet de visualisation, avec des cartes (dont certaines sont interactives) et une vidéo. Les principaux projets sont disponibles dans l’onglet éponyme."
  },
  {
    "objectID": "index.html#stations-de-vélos-à-montpellier.",
    "href": "index.html#stations-de-vélos-à-montpellier.",
    "title": "Visualisation des trajets en vélo dans la ville de Montpellier.",
    "section": "Stations de vélos à Montpellier.",
    "text": "Stations de vélos à Montpellier."
  },
  {
    "objectID": "index.html#test-importation-fichier-html",
    "href": "index.html#test-importation-fichier-html",
    "title": "Visualisation des trajets en vélo dans la ville de Montpellier.",
    "section": "Test importation fichier HTML",
    "text": "Test importation fichier HTML"
  },
  {
    "objectID": "index.html#test-importation-fichier-python",
    "href": "index.html#test-importation-fichier-python",
    "title": "Visualisation des trajets en vélo dans la ville de Montpellier.",
    "section": "Test Importation fichier Python",
    "text": "Test Importation fichier Python\n\n\nNombre de trajets par jour:\n           Date  Nombre de trajets\n0    2023-12-01                241\n1    2023-12-02                331\n2    2023-12-03                262\n3    2023-12-04                175\n4    2023-12-05                272\n..          ...                ...\n241  2024-09-26                609\n242  2024-09-27                682\n243  2024-09-28                679\n244  2024-09-29                742\n245  2024-09-30                567\n\n[246 rows x 2 columns]\n\n\n\n\n\n\n\n\n\nIndex([23, 23, 23, 23, 23, 23, 23, 22, 22, 22,\n       ...\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n      dtype='int32', name='Departure', length=126786)"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Visualisation des trajets en vélo dans la ville de Montpellier.",
    "section": "Contact",
    "text": "Contact\nPour toute question ou suggestion, n’hésitez pas à nous contacter."
  },
  {
    "objectID": "projet2.html",
    "href": "projet2.html",
    "title": "Projet 2",
    "section": "",
    "text": "Ici, les cartes issues de nos principaux travaux.\n\n\nNombre de trajets par jour:\n           Date  Nombre de trajets\n0    2023-12-01                242\n1    2023-12-02                338\n2    2023-12-03                263\n3    2023-12-04                178\n4    2023-12-05                281\n..          ...                ...\n241  2024-09-26                612\n242  2024-09-27                684\n243  2024-09-28                681\n244  2024-09-29                743\n245  2024-09-30                583\n\n[246 rows x 2 columns]\n\n\n\n\n\n\n\n\n\nIndex([23, 23, 23, 23, 23, 23, 23, 22, 22, 22,\n       ...\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n      dtype='int32', name='Departure', length=128504)\n\n\n                                                \n\n\nNombre de trajets par jour:\n           Date  Nombre de trajets\n0    2020-11-01                323\n1    2020-11-02                404\n2    2020-11-03                463\n3    2020-11-04                450\n4    2020-11-05                458\n..          ...                ...\n447  2022-01-27                813\n448  2022-01-28                864\n449  2022-01-29                844\n450  2022-01-30                973\n451  2022-01-31                643\n\n[452 rows x 2 columns]\n\n\n\n\n\n\n\n\n\nIndex([23, 23, 23, 23, 23, 22, 22, 22, 22, 22,\n       ...\n        1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n      dtype='int32', name='Departure', length=403320)\n\n\n                                                \n\n\nNombre de trajets par jour:\n           Date  Nombre de trajets\n0    2021-09-01               1227\n1    2021-09-02               1310\n2    2021-09-03                905\n3    2021-09-04               1390\n4    2021-09-05               1465\n..          ...                ...\n359  2022-12-27                310\n360  2022-12-28                226\n361  2022-12-29                283\n362  2022-12-30                300\n363  2022-12-31                317\n\n[364 rows x 2 columns]\n\n\n\n\n\n\n\n\n\nIndex([23, 23, 23, 23, 23, 23, 23, 23, 23, 22,\n       ...\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n      dtype='int32', name='Departure', length=347157)\n\n\n                                                \n\n\nNombre de trajets par jour:\n           Date  Nombre de trajets\n0    2023-01-01                521\n1    2023-01-02                252\n2    2023-01-03                295\n3    2023-01-04                384\n4    2023-01-05                459\n..          ...                ...\n329  2023-11-26                371\n330  2023-11-27                304\n331  2023-11-28                311\n332  2023-11-29                325\n333  2023-11-30                361\n\n[334 rows x 2 columns]\n\n\n\n\n\n\n\n\n\nIndex([23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n       ...\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n      dtype='int32', name='Departure', length=235544)\n\n\n                                                \n\n\nNombre de trajets par jour:\n           Date  Nombre de trajets\n0    2023-01-01                521\n1    2023-01-02                252\n2    2023-01-03                295\n3    2023-01-04                384\n4    2023-01-05                459\n..          ...                ...\n329  2023-11-26                371\n330  2023-11-27                304\n331  2023-11-28                311\n332  2023-11-29                325\n333  2023-11-30                361\n\n[334 rows x 2 columns]\n\n\n\n\n\n\n\n\n\nIndex([23, 23, 23, 23, 23, 23, 23, 23, 23, 23,\n       ...\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n      dtype='int32', name='Departure', length=235544)"
  },
  {
    "objectID": "auteurs.html#auteurs",
    "href": "auteurs.html#auteurs",
    "title": "Contributeurs",
    "section": "",
    "text": "Email\nGitHub Profile\n\n\n\n\n\nEmail\nGitHub Profile\n\n\n\n\n\nEmail\nGitHub Profile\n\n\n\n\n\nEmail\nGitHub Profile"
  },
  {
    "objectID": "docu.html#contexte-et-objectifs",
    "href": "docu.html#contexte-et-objectifs",
    "title": "Documentation",
    "section": "",
    "text": "Le développement de ce projet s’inscrit dans un contexte où les enjeux de mobilité durable et de gestion intelligente des infrastructures urbaines sont de plus en plus cruciaux. L’augmentation de la fréquentation cycliste dans les grandes villes, associée à une volonté d’encourager les comportements écoresponsables, nécessite des outils adaptés pour analyser et prédire les flux de circulation. Ce projet, à travers son site web interactif, cherche à répondre à ces besoins en fournissant des visualisations précises et en temps réel du trafic cycliste."
  },
  {
    "objectID": "docu.html#structure-du-projet",
    "href": "docu.html#structure-du-projet",
    "title": "Documentation",
    "section": "2.1 Structure du projet",
    "text": "2.1 Structure du projet\nLa structure du projet est organisée de manière à assurer une séparation claire des différentes composantes nécessaires au bon déroulement de l’analyse et à la création des visualisations. Voici un aperçu de l’organisation des fichiers et répertoires :\n\n\nVoir le structure\n\nmain/                                           # Répertoire principal du projet\n├── .github/workflows/                          # \n├── analyse_donnee/                             # \n├── cache/                                      # Fichiers temporaires\n├── data/                                       # Stockage des données des 3 jeux de données\n│   ├── extracted/                              # \n│   ├── CoursesVelomagg.csv                     # Données sur les trajets de vélos en libre-service\n│   ├── video.csv                               # \n│   └── video_avec_coord.csv                    # \n├── docs/                                       # Répertoire des fichiers du site web\n│   ├── .gitignore                              # Liste des fichiers à ignorer par Git\n│   ├──                                         # \n│   ├──                                         # \n│   ├──                                         # \n│   └── styles.css                              # Fichier CSS pour la mise en forme du site\n├── images/                                     # Images utilisées dans le projet\n├── map/                                        # \n├── roadmap/                                    # Fichier README.md avec la description .......\n├── slide/                                      # \n├── src/                                        # Répertoire contenant le code source\n│   ├── __init__.py                             # Initialisation du package\n│   ├── donnée.py                               # Contient les fonctions liées aux données\n│   └── fonctions_basedonnees.py                # Fonctions pour les opérations sur la base de données\n├── vidéo/                                      # \n├── visualisation/                              # Résultats des visualisations\n├──.DS_Store                                    # \n├──\n├──\n├── .Rhistory                                   # Fichier d'historique R\n├── .gitignore                                  # Fichier Git ignore\n├── README.md                                   # Documentation principale du projet\n└── requirements.txt                            # Dépendances du projet"
  },
  {
    "objectID": "docu.html#installation-et-dépendances",
    "href": "docu.html#installation-et-dépendances",
    "title": "Documentation",
    "section": "2.2 Installation et dépendances",
    "text": "2.2 Installation et dépendances\nAvant de commencer le développement et l’analyse des données, il est nécessaire de configurer l’environnement de travail en installant toutes les dépendances nécessaires. Voici les étapes à suivre pour préparer le projet :\n\nCloner le dépôt Git : La première étape consiste à cloner le dépôt du projet depuis GitHub à l’aide de la commande suivante :\ngit clone https://github.com/mscaia/PROJ_HAX712X.git\nInstaller les dépendances : Le projet repose sur plusieurs bibliothèques Python pour l’analyse des données, la création de visualisations interactives et la génération du site web. Pour installer ces bibliothèques, utilisez la commande suivante :\npip install -r requirements.txt\nVérification de l’environnement : Après l’installation des dépendances, vous pouvez vérifier que tout fonctionne correctement en exécutant un script de test ou en accédant au site web via quarto :\nquarto preview\nCela lancera un serveur local où vous pourrez visualiser le site et tester les fonctionnalités interactives."
  },
  {
    "objectID": "docu.html#configuration-des-fichiers",
    "href": "docu.html#configuration-des-fichiers",
    "title": "Documentation",
    "section": "2.3 Configuration des fichiers",
    "text": "2.3 Configuration des fichiers\nLe projet est configuré de manière à utiliser Quarto pour la création du site web interactif et Sphinx pour la documentation générée automatiquement. Le fichier _quarto.yml contient les paramètres de configuration du projet Quarto, tandis que le fichier index.qmd contient le contenu principal de la page d’accueil du site."
  },
  {
    "objectID": "docu.html#base-de-données-et-traitement-des-données",
    "href": "docu.html#base-de-données-et-traitement-des-données",
    "title": "Documentation",
    "section": "2.4 Base de données et traitement des données",
    "text": "2.4 Base de données et traitement des données\nLe répertoire data/ contient les trois principaux jeux de données utilisés dans ce projet :\n\nCoursesVelomagg.csv : Contient les informations sur les trajets effectués avec les vélos en libre-service.\nEcoCompt1.json : Données de comptage des cyclistes et piétons provenant des capteurs installés dans la ville.\nGeolocCompteurs.csv : Données géospatiales pour localiser les capteurs de comptage des vélos et piétons.\n\nLe traitement de ces données est effectué dans le répertoire src/, où les scripts comme donnée.py et fonctions_basedonnees.py contiennent les fonctions nécessaires pour charger, nettoyer et manipuler les données."
  },
  {
    "objectID": "docu.html#exécution-du-projet",
    "href": "docu.html#exécution-du-projet",
    "title": "Documentation",
    "section": "2.5 Exécution du projet",
    "text": "2.5 Exécution du projet\nUne fois l’environnement configuré et les données traitées, il suffit de lancer le script principal pour générer les visualisations interactives et les cartes prédictives du trafic cycliste. Le projet utilise Folium pour la visualisation sur carte et Matplotlib pour les graphiques statistiques.\nLe répertoire visualisation/ contient les scripts et sorties graphiques générées au fur et à mesure du traitement des données, notamment les cartes des trajets cyclistes et les prévisions de trafic."
  },
  {
    "objectID": "docu.html#documentation-et-tests",
    "href": "docu.html#documentation-et-tests",
    "title": "Documentation",
    "section": "2.6 Documentation et tests",
    "text": "2.6 Documentation et tests\nPour garantir la qualité du code et la conformité aux exigences du projet, des tests unitaires sont fournis pour vérifier le bon fonctionnement des différentes fonctionnalités. La documentation technique est générée via Sphinx et Quarto, offrant ainsi une vue d’ensemble sur la structure du code, les API disponibles et la méthodologie utilisée."
  },
  {
    "objectID": "docu.html#bibliothèques-utilisées",
    "href": "docu.html#bibliothèques-utilisées",
    "title": "Documentation",
    "section": "6.1 Bibliothèques utilisées",
    "text": "6.1 Bibliothèques utilisées\nDans le cadre de ce projet, plusieurs bibliothèques ont été utilisées pour répondre aux différents besoins techniques et analytiques. Voici une présentation des bibliothèques principales et leur rôle.\n\n\ncsv\n\nLa bibliothèque csv permet de lire et d’écrire des fichiers CSV, un format commun pour manipuler des données tabulaires. Nous avons utilisé csv pour extraire et traiter les données brutes contenues dans des fichiers au format CSV. Cela est particulièrement utile pour manipuler des ensembles de données simples où une lecture ligne par ligne est nécessaire.\n\n\n\nmatplotlib.pyplot\n\nmatplotlib.pyplot est utilisée pour produire des graphiques statiques et des visualisations animées. Dans ce projet, elle permet de créer des graphiques dynamiques illustrant les trajectoires cyclistes et d’exporter ces visualisations sous forme de vidéos à l’aide des modules FuncAnimation et FFMpegWriter.\n\n\n\nmatplotlib.animation.FuncAnimation et FFMpegWriter\n\nCes modules de la bibliothèque matplotlib.animation permettent de créer des animations et d’exporter celles-ci sous forme de fichiers vidéo. Dans le projet, ils sont utilisés pour générer des animations illustrant les variations temporelles du trafic cycliste et les enregistrer sous un format visuel accessible.\n\n\n\nnumpy\n\nnumpy est une bibliothèque puissante pour effectuer des calculs numériques avancés, notamment des opérations matricielles. Les opérations matricielles et les calculs numériques complexes nécessaires à l’analyse des données sont simplifiés grâce à numpy, qui garantit également des performances élevées.\n\n\n\nos\n\nos fournit des fonctions pour interagir avec le système d’exploitation, notamment pour gérer les fichiers et les répertoires. Nous avons utilisé os pour gérer les chemins des fichiers, vérifier l’existence des répertoires, et manipuler les ressources locales du système.\n\n\n\npooch\n\npooch facilite le téléchargement et la mise en cache des fichiers nécessaires à l’exécution du projet. Cette bibliothèque permet de garantir un accès fiable aux données externes en les téléchargeant automatiquement et en les stockant localement pour une réutilisation future.\n\n\n\npandas\n\npandas est essentielle pour manipuler et analyser des données tabulaires de manière efficace. pandas est utilisée pour nettoyer, transformer et analyser des ensembles de données complexes, offrant des fonctionnalités avancées comme le traitement des séries temporelles et des jointures de tables.\n\n\n\njson\n\njson est utilisée pour manipuler des données au format JSON, un standard de stockage et d’échange d’informations structurées. Nous utilisons json pour lire et écrire des données structurées, notamment pour gérer les configurations et les résultats intermédiaires dans des fichiers légers.\n\n\n\nre\n\nLa bibliothèque re permet de travailler avec des expressions régulières pour manipuler des chaînes de caractères. Grâce à re, nous avons pu extraire des informations spécifiques des chaînes de caractères et nettoyer les données textuelles de manière efficace.\n\n\n\nunicodedata\n\nunicodedata est utilisée pour normaliser les chaînes de caractères Unicode. Cette bibliothèque est essentielle pour traiter les caractères spéciaux et garantir la cohérence des chaînes de caractères provenant de différentes sources.\n\n\n\nfolium\n\nfolium est une bibliothèque dédiée à la création de cartes interactives. Nous avons utilisé folium pour visualiser les trajets et itinéraires des vélos sur des cartes interactives, permettant une meilleure compréhension géographique des données.\n\n\n\nosmnx\n\nosmnx est utilisée pour le géocodage et l’analyse des réseaux géographiques. Cette bibliothèque permet d’extraire des données géographiques d’OpenStreetMap, de construire des graphes routiers, et d’analyser les itinéraires et les trajets cyclistes dans le cadre de ce projet.\n\n\n\nfunctools.lru_cache\n\nfunctools.lru_cache est une fonctionnalité de Python pour optimiser les performances. En mettant en cache les résultats des fonctions fréquemment appelées, functools.lru_cache améliore les performances et réduit le temps de calcul pour des opérations répétées.\n\n\n\nnetworkx\n\nnetworkx est une bibliothèque dédiée à la création, la manipulation et l’analyse de graphes complexes. Dans ce projet, elle est utilisée pour représenter et étudier les réseaux cyclistes, notamment pour visualiser les trajets et calculer les chemins les plus courts entre les nœuds.\n\n\n\nconcurrent.futures.ThreadPoolExecutor\n\nThreadPoolExecutor est une fonctionnalité du module standard concurrent.futures pour exécuter des tâches en parallèle. Elle est utilisée pour optimiser le traitement des données et accélérer le rendu des animations dans le cadre de ce projet.\n\n\n\ndatetime\n\ndatetime est un module intégré pour manipuler les dates et les heures. Dans le projet, il est employé pour traiter les données temporelles des trajets cyclistes et synchroniser les animations avec les horodatages."
  },
  {
    "objectID": "docu.html#functions",
    "href": "docu.html#functions",
    "title": "Documentation",
    "section": "6.2 Functions",
    "text": "6.2 Functions\n\n6.2.1 Map\n\n\ncolonne(i, w_file)\n\nDescription :\nCette fonction permet d’extraire une colonne spécifique d’un fichier CSV. Elle prend en entrée un indice i, représentant la colonne à extraire, ainsi que le chemin du fichier w_file. La fonction ouvre le fichier, parcourt chaque ligne et récupère l’élément situé à la position i dans chaque ligne. Le résultat est une liste contenant toutes les valeurs de la colonne souhaitée.\nParamètres :\n\n\ni (int) : L’indice de la colonne à extraire.\n\n\nw_file (str) : Le chemin d’accès au fichier CSV.\n\n\nRetourne :\n\n\nL (list) : Une liste contenant les valeurs de la colonne spécifiée.\n\n\nCode de la fonction :\n  def colonne(i, w_file):\n    L=[]\n    with open(w_file) as f:\n        for line in f:\n            x=line.split(\",\")\n            L.append(x[i])\n    return L \n\n\n\narg(k, i, j, w_file)\n\nDescription :\nCette fonction retourne toutes les valeurs dans la colonne j lorsque l’argument dans la colonne i est égal à k. Elle est utile pour filtrer un ensemble de données en fonction d’une condition donnée dans une colonne spécifique, puis extraire les valeurs correspondantes dans une autre colonne.\nParamètres :\n\n\nk (str) : La valeur de la colonne i que l’on souhaite rechercher.\n\n\ni (int) : L’indice de la colonne à vérifier.\n\n\nj (int) : L’indice de la colonne dont les valeurs seront extraites.\n\n\nw_file (str) : Le chemin d’accès au fichier CSV.\n\n\nRetourne :\n\n\nL (list) : Une liste des valeurs de la colonne j correspondant à la condition x[i] == k.\n\n\nCode de la fonction :\n  def arg(k,i,j, w_file):\n    L=[]\n    with open(w_file) as f:\n        for line in f:\n            x=line.split(\",\")\n            if x[i]==k:\n                L.append(x[j])\n    return L \n\n\n\npd_to_datetime(df, colonne_date)\n\nDescription :\nCette fonction transforme une colonne de dates dans un DataFrame en un format exploitable. Elle supprime les valeurs manquantes, convertit la colonne de dates en format datetime et crée une nouvelle colonne Date contenant uniquement la date, en supprimant la colonne initiale des dates.\nParamètres :\n\n\ndf (DataFrame) : Le DataFrame contenant la colonne de dates.\n\n\ncolonne_date (str) : Le nom de la colonne de dates à convertir.\n\n\nRetourne :\n\n\ndf (DataFrame) : Le DataFrame modifié avec la colonne Date et la colonne initiale supprimée.\n\n\nCode de la fonction :\n  def pd_to_datetime(df, colonne_date):\n    df = df.dropna()\n    df[colonne_date] = pd.to_datetime(df[colonne_date])\n    df['Date'] = df[colonne_date].dt.date\n    df = df.drop(columns=[colonne_date])\n    return df\n\n\n\nnettoyer_adresse_normalise(adresse)\n\nDescription :\nCette fonction nettoie et normalise une adresse en supprimant les numéros au début et en corrigeant les caractères Unicode. Elle utilise des expressions régulières pour enlever les numéros de début d’adresse et applique une normalisation Unicode pour garantir une uniformité des caractères.\nParamètres :\n\n\nadresse (str) : La chaîne de caractères représentant l’adresse à nettoyer et normaliser.\n\n\nRetourne :\n\n\nstr : L’adresse nettoyée et normalisée.\n\n\nCode de la fonction :\n  def nettoyer_adresse_normalise(adresse):\n    \"\"\"\n    Nettoie et normalise une adresse en supprimant les numéros au début, \n    en normalisant les caractères Unicode.\n    \n    Paramètre :\n    adresse (str) : La chaîne d'adresse à normaliser.\n    \n    Retourne :\n    str : L'adresse nettoyée et normalisée.\n    \"\"\"\n    # Tenter de corriger l'encodage si nécessaire\n    try:\n        # Encode la chaîne en latin1 puis décode en utf-8\n        adresse = adresse.encode('latin1').decode('utf-8')\n    except (UnicodeEncodeError, UnicodeDecodeError):\n        pass  # Ignore les erreurs d'encodage si elles se produisent\n\n    # Supprimer les numéros ou autres formats non pertinents (ex: 057 au début)\n    adresse = re.sub(r'^\\d+\\s*', '', adresse)  # Enlève les numéros au début\n    \n    # Normalisation des caractères Unicode\n    adresse = unicodedata.normalize('NFKD', adresse)\n    \n    # Retourner l'adresse nettoyée et normalisée\n    return adresse  # Enlever les espaces supplémentaires aux extrémités\n\n\n\ngen_carte_trajet(ligne, G, m, index_colonne_départ, index_colonne_arrive, couleur)\n\nDescription :\nCette fonction génère une carte représentant le trajet entre deux points de départ et d’arrivée. Elle utilise le géocodage pour convertir les noms des stations en coordonnées géographiques, puis calcule l’itinéraire le plus court entre les deux stations sur un graphe de rue. Les itinéraires sont ajoutés à la carte, ainsi que des marqueurs pour les stations de départ et d’arrivée.\nParamètres :\n\n\nligne (list) : Une ligne contenant les noms des stations de départ et d’arrivée.\n\n\nG (Graph) : Le graphe représentant le réseau de rues de la ville.\n\n\nm (Map) : L’objet de la carte sur lequel le trajet sera tracé.\n\n\nindex_colonne_départ (int) : L’indice de la colonne contenant le nom de la station de départ.\n\n\nindex_colonne_arrive (int) : L’indice de la colonne contenant le nom de la station d’arrivée.\n\n\ncouleur (str) : La couleur de la ligne représentant le trajet.\n\n\nRetourne :\n\n\nm (Map) : La carte avec l’itinéraire ajouté.\n\n\nCode de la fonction :\n  def gen_carte_trajet(ligne, G, m, index_colonne_départ, index_colonne_arrive,couleur):\n    # Essayer de géocoder les stations de départ et d'arrivée\n    try:\n        origin = ox.geocode(f\"{ligne[index_colonne_départ]}, Montpellier, France\")  # Première colonne\n        destination = ox.geocode(f\"{ligne[index_colonne_arrive]}, Montpellier, France\")  # Deuxième colonne\n        \n        # Vérifier si le géocodage a réussi\n        if origin is None or destination is None:\n            print(f\"Erreur de géocodage pour les stations : {ligne[index_colonne_départ]} ou {ligne[index_colonne_arrive]}\")\n            return m\n        \n        # Trouver les nœuds les plus proches de l'origine et de la destination\n        origin_node = ox.nearest_nodes(G, origin[1], origin[0])  # longitude, latitude\n        destination_node = ox.nearest_nodes(G, destination[1], destination[0])  # longitude, latitude\n\n        # Calculer l'itinéraire aller et retour\n        route = ox.shortest_path(G, origin_node, destination_node)\n\n        # Fonction pour convertir un itinéraire (liste de nœuds) en liste de coordonnées géographiques\n        def route_to_coords(G, route):\n            route_coords = []\n            for node in route:\n                point = (G.nodes[node]['y'], G.nodes[node]['x'])  # latitude, longitude\n                route_coords.append(point)\n            return route_coords\n\n        # Obtenir les coordonnées pour l'itinéraire\n        route_coords = route_to_coords(G, route)\n\n        # Ajouter l'itinéraire aller (en rouge) à la carte\n        folium.PolyLine(locations=route_coords, color=couleur, weight=5, opacity=0.75).add_to(m)\n\n        # Ajouter des marqueurs pour l'origine et la destination\n        départ_lat, départ_lon = route_coords[0]\n        arr_lat, arr_lon = route_coords[-1]  # Utiliser le dernier point pour l'arrivée\n        folium.Marker(location=[départ_lat, départ_lon], popup=f\"{ligne[index_colonne_départ]},Départ\").add_to(m)\n        folium.Marker(location=[arr_lat, arr_lon], popup=f\"{ligne[index_colonne_arrive]},arrivé\").add_to(m)\n\n    except Exception as e:\n        print(f\"Une erreur est survenue : {e}\")\n    \n    return m\n\n\n\ncoordonne(station)\n\nDescription :\nCette fonction permet de géocoder le nom d’une station pour obtenir ses coordonnées géographiques (latitude et longitude). Elle utilise la bibliothèque osmnx pour rechercher l’emplacement correspondant à la station spécifiée dans la ville de Montpellier, France.\nParamètres :\n\n\nstation (str) : Le nom de la station à géocoder.\n\n\nRetourne :\n\n\nlatitude (float) : La latitude de la station.\n\n\nlongitude (float) : La longitude de la station.\n\n\nCode de la fonction :\n  def coordonne(station):\n    try:\n        # Recherche de l'emplacement en utilisant osmnx\n        location = ox.geocode(f\"{station}, Montpellier, France\")\n        return location[0], location[1]\n    except Exception as e:\n        print(f\"Erreur pour la station {station}: {e}\")\n        return None, None\n\n\n\n6.2.2 Préduction [DOCUM FROM CA - PUT HERE]\n\n\n6.2.3 Video [VIDEO FUNCTION]"
  },
  {
    "objectID": "docu.html#class",
    "href": "docu.html#class",
    "title": "Documentation",
    "section": "6.3 Class",
    "text": "6.3 Class\n[HERE IS A TEXT ABOUT CLASS]"
  },
  {
    "objectID": "docu.html#structure-du-projet-will-be-finished-at-the-end",
    "href": "docu.html#structure-du-projet-will-be-finished-at-the-end",
    "title": "Documentation",
    "section": "2.1 Structure du projet [WILL BE FINISHED AT THE END]",
    "text": "2.1 Structure du projet [WILL BE FINISHED AT THE END]\nLa structure du projet est organisée de manière à assurer une séparation claire des différentes composantes nécessaires au bon déroulement de l’analyse et à la création des visualisations. Voici un aperçu de l’organisation des fichiers et répertoires :\n\n\nVoir le structure\n\nmain/                                           # Répertoire principal du projet\n├── .github/workflows/                          # \n├── analyse_donnee/                             # \n├── cache/                                      # Fichiers temporaires\n├── data/                                       # Stockage des données des 3 jeux de données\n│   ├── extracted/                              # \n│   ├── CoursesVelomagg.csv                     # Données sur les trajets de vélos en libre-service\n│   ├── video.csv                               # \n│   └── video_avec_coord.csv                    # \n├── docs/                                       # Répertoire des fichiers du site web\n│   ├── .gitignore                              # Liste des fichiers à ignorer par Git\n│   ├──                                         # \n│   ├──                                         # \n│   ├──                                         # \n│   └── styles.css                              # Fichier CSS pour la mise en forme du site\n├── images/                                     # Images utilisées dans le projet\n├── map/                                        # \n├── roadmap/                                    # Fichier README.md avec la description .......\n├── slide/                                      # \n├── src/                                        # Répertoire contenant le code source\n│   ├── __init__.py                             # Initialisation du package\n│   ├── donnée.py                               # Contient les fonctions liées aux données\n│   └── fonctions_basedonnees.py                # Fonctions pour les opérations sur la base de données\n├── vidéo/                                      # \n├── visualisation/                              # Résultats des visualisations\n├──.DS_Store                                    # \n├──\n├──\n├── .Rhistory                                   # Fichier d'historique R\n├── .gitignore                                  # Fichier Git ignore\n├── README.md                                   # Documentation principale du projet\n└── requirements.txt                            # Dépendances du projet"
  },
  {
    "objectID": "docu.html#configuration-des-fichiers-sphinx---cheked",
    "href": "docu.html#configuration-des-fichiers-sphinx---cheked",
    "title": "Documentation",
    "section": "2.3 Configuration des fichiers [Sphinx - CHEKED]",
    "text": "2.3 Configuration des fichiers [Sphinx - CHEKED]\nLe projet est configuré de manière à utiliser Quarto pour la création du site web interactif et [Sphinx pour la documentation générée automatiquement]. Le fichier _quarto.yml contient les paramètres de configuration du projet Quarto, tandis que le fichier index.qmd contient le contenu principal de la page d’accueil du site."
  },
  {
    "objectID": "docu.html#base-de-données-et-traitement-des-données-should-be-recheked-after",
    "href": "docu.html#base-de-données-et-traitement-des-données-should-be-recheked-after",
    "title": "Documentation",
    "section": "2.4 Base de données et traitement des données [SHOULD BE RECHEKED AFTER]",
    "text": "2.4 Base de données et traitement des données [SHOULD BE RECHEKED AFTER]\nLe répertoire data/ contient les trois principaux jeux de données utilisés dans ce projet :\n\nCoursesVelomagg.csv : Contient les informations sur les trajets effectués avec les vélos en libre-service.\nvideo.csv :\nvideo_avec_coord.csv :\n\nLe traitement de ces données est effectué dans le répertoire src/, où les scripts comme donnée.py et fonctions_basedonnees.py contiennent les fonctions nécessaires pour charger, nettoyer et manipuler les données."
  },
  {
    "objectID": "docu.html#documentation-et-tests-finish-and-rechecked",
    "href": "docu.html#documentation-et-tests-finish-and-rechecked",
    "title": "Documentation",
    "section": "2.6 Documentation et tests [FINISH AND RECHECKED]",
    "text": "2.6 Documentation et tests [FINISH AND RECHECKED]\nPour garantir la qualité du code et la conformité aux exigences du projet, des tests unitaires sont fournis pour vérifier le bon fonctionnement des différentes fonctionnalités. La documentation technique est générée via Sphinx et Quarto, offrant ainsi une vue d’ensemble sur la structure du code, les API disponibles et la méthodologie utilisée."
  },
  {
    "objectID": "docu.html#acquisition-des-données",
    "href": "docu.html#acquisition-des-données",
    "title": "Documentation",
    "section": "5.1 Acquisition des données",
    "text": "5.1 Acquisition des données\nLes ensembles de données ont été collectés auprès de sources officielles :\n\nDonnées VéloMagg : Historique des trajets effectués via le système de vélos en libre-service.\nComptages cyclistes et piétons : Relevés par des capteurs aux points stratégiques de la ville.\nDonnées OpenStreetMap (OSM) : Informations géographiques pour la cartographie.\n\nCes fichiers ont été extraits au format .csv et .json et chargés à l’aide de la bibliothèque pandas. Une validation initiale a été effectuée pour vérifier l’intégrité des fichiers (taille, colonnes attendues, etc.)."
  },
  {
    "objectID": "docu.html#prétraitement",
    "href": "docu.html#prétraitement",
    "title": "Documentation",
    "section": "5.2 Prétraitement",
    "text": "5.2 Prétraitement\nL’étape de prétraitement vise à préparer les données pour l’analyse. Les principales opérations incluent :\n\n\nNettoyage des données : Suppression des valeurs manquantes et des doublons.\n\n\nTransformation des variables : Normalisation des données temporelles pour harmoniser les différents jeux de données.\n\n\nFiltrage géographique : Limitation aux trajets effectués dans Montpellier."
  },
  {
    "objectID": "docu.html#analyse-exploratoire",
    "href": "docu.html#analyse-exploratoire",
    "title": "Documentation",
    "section": "5.3 Analyse exploratoire",
    "text": "5.3 Analyse exploratoire\nUne exploration préliminaire a permis de dégager des tendances :\n\n\nAnalyse temporelle des flux de cyclistes (par heure, jour, mois).\n\n\nCartographie des zones à fort trafic à l’aide de Folium.\n\n\nVisualisation des variations de densité de trafic.\n\n\nCes étapes ont permis d’orienter les choix algorithmiques pour la prédiction."
  },
  {
    "objectID": "docu.html#modélisation",
    "href": "docu.html#modélisation",
    "title": "Documentation",
    "section": "5.4 Modélisation",
    "text": "5.4 Modélisation\nUn algorithme de prédiction du trafic a été conçu en se basant sur des variables clés :\n\n\nDonnées historiques des trajets.\n\n\nLocalisation et fréquence des trajets.\n\n\nInfluence potentielle des heures de pointe et des jours fériés.\n\n\nL’implémentation repose sur des modèles de régression supervisée. Une attention particulière a été portée à la validation croisée pour assurer la fiabilité des prédictions."
  },
  {
    "objectID": "docu.html#visualisation",
    "href": "docu.html#visualisation",
    "title": "Documentation",
    "section": "5.5 Visualisation",
    "text": "5.5 Visualisation\nLes résultats sont intégrés dans des visualisations interactives :\n\n\nCartes dynamiques illustrant les trajets par zones.\n\n\nTableau de bord synthétisant les flux par période.\n\n\nL’ensemble de ces visualisations est hébergé sur le site web interactif du projet."
  },
  {
    "objectID": "docu.html#intégration",
    "href": "docu.html#intégration",
    "title": "Documentation",
    "section": "5.6 Intégration",
    "text": "5.6 Intégration\nLe pipeline est conçu pour être adaptable :\n\n\nPortabilité : Facilité d’application à d’autres villes disposant de systèmes similaires.\n\n\nScalabilité : Support de nouveaux ensembles de données (ex. pollution ou météo)."
  },
  {
    "objectID": "docu.html#perspectives",
    "href": "docu.html#perspectives",
    "title": "Documentation",
    "section": "Perspectives",
    "text": "Perspectives\nPour enrichir le pipeline, des pistes futures incluent l’intégration de données en temps réel et l’optimisation de la performance pour traiter des volumes massifs de données."
  }
]